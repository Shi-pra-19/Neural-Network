{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOW3iSGg2zMi"
      },
      "source": [
        "#Import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AILN2IWOogi-",
        "outputId": "18f8edd7-3a03-496a-f275-b5514bd02607"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /kaggle/input/goodreads-best-books\n",
            "Index(['book_authors', 'book_desc', 'book_edition', 'book_format', 'book_isbn',\n",
            "       'book_pages', 'book_rating', 'book_rating_count', 'book_review_count',\n",
            "       'book_title', 'genres', 'image_url'],\n",
            "      dtype='object')\n",
            "                                        book_authors  \\\n",
            "0                                    Suzanne Collins   \n",
            "1                         J.K. Rowling|Mary GrandPré   \n",
            "2                                         Harper Lee   \n",
            "3  Jane Austen|Anna Quindlen|Mrs. Oliphant|George...   \n",
            "4                                    Stephenie Meyer   \n",
            "\n",
            "                                           book_desc  \\\n",
            "0  Winning will make you famous. Losing means cer...   \n",
            "1  There is a door at the end of a silent corrido...   \n",
            "2  The unforgettable novel of a childhood in a sl...   \n",
            "3  «È cosa ormai risaputa che a uno scapolo in po...   \n",
            "4  About three things I was absolutely positive.F...   \n",
            "\n",
            "                         book_edition book_format    book_isbn book_pages  \\\n",
            "0                                 NaN   Hardcover  9.78044E+12  374 pages   \n",
            "1                          US Edition   Paperback  9.78044E+12  870 pages   \n",
            "2                    50th Anniversary   Paperback  9.78006E+12  324 pages   \n",
            "3  Modern Library Classics, USA / CAN   Paperback  9.78068E+12  279 pages   \n",
            "4                                 NaN   Paperback  9.78032E+12  498 pages   \n",
            "\n",
            "   book_rating  book_rating_count  book_review_count  \\\n",
            "0         4.33            5519135             160706   \n",
            "1         4.48            2041594              33264   \n",
            "2         4.27            3745197              79450   \n",
            "3         4.25            2453620              54322   \n",
            "4         3.58            4281268              97991   \n",
            "\n",
            "                                  book_title  \\\n",
            "0                           The Hunger Games   \n",
            "1  Harry Potter and the Order of the Phoenix   \n",
            "2                      To Kill a Mockingbird   \n",
            "3                        Pride and Prejudice   \n",
            "4                                   Twilight   \n",
            "\n",
            "                                              genres  \\\n",
            "0  Young Adult|Fiction|Science Fiction|Dystopia|F...   \n",
            "1                        Fantasy|Young Adult|Fiction   \n",
            "2  Classics|Fiction|Historical|Historical Fiction...   \n",
            "3                           Classics|Fiction|Romance   \n",
            "4  Young Adult|Fantasy|Romance|Paranormal|Vampire...   \n",
            "\n",
            "                                           image_url  \n",
            "0  https://images.gr-assets.com/books/1447303603l...  \n",
            "1  https://images.gr-assets.com/books/1255614970l...  \n",
            "2  https://images.gr-assets.com/books/1361975680l...  \n",
            "3  https://images.gr-assets.com/books/1320399351l...  \n",
            "4  https://images.gr-assets.com/books/1361039443l...  \n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download\n",
        "path = kagglehub.dataset_download(\"meetnaren/goodreads-best-books\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "csv_file = os.path.join(path, \"book_data.csv\")\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "print(df.columns)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbpJ6gU93Aep"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NB4XQSK_pRD_",
        "outputId": "e78f5108-91bd-417c-f264-503fc56f5dd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "31315    Paranormal|Fairies|Fantasy|Magic|Fantasy|Child...\n",
            "234      Fantasy|Fiction|Romance|Historical|Historical ...\n",
            "49509    Classics|Poetry|Fantasy|Mythology|Religion|Fic...\n",
            "9661          Fiction|Contemporary|Literary Fiction|Novels\n",
            "38379                                              Fiction\n",
            "4992                                          Spirituality\n",
            "29019      Fantasy|Paranormal|Holiday|Contemporary|Romance\n",
            "53029    Historical|Historical Fiction|Fiction|Historic...\n",
            "31357                              Religion|Islam|Religion\n",
            "42960    Fantasy|Paranormal|Paranormal|Vampires|Romance...\n",
            "Name: genres, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(df['genres'].sample(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4z0U5XacxmlT"
      },
      "outputs": [],
      "source": [
        "#df= df.sample(5000, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc0z4SZcpZaP"
      },
      "outputs": [],
      "source": [
        "# Split by '|' and keep first genre\n",
        "df['genre'] = df['genres'].str.split('|').str[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikAtqg5jpgGT",
        "outputId": "77262a3f-6e9b-4fcf-ceda-9235a879aa15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                               book_desc       genre\n",
            "9302   Once she was Adrienne Satti. An orphan of Davi...     Fantasy\n",
            "38566  Denver is rich - very, very rich. Everyone in ...   Childrens\n",
            "44024  Transcription of the handwritten pages:http://...  Historical\n",
            "24279  Comedian Gabbie Hanna brings levity to the twi...      Poetry\n",
            "27322  It's almost a year since Gaby Winters watched ...  Paranormal\n",
            "46776  A commentary on contemporary urban mores and m...       Plays\n",
            "51962  A young man from a small provincial town moves...   Biography\n",
            "6495   A Song of Love won her heart.A Song of Darknes...     Fantasy\n",
            "12782  We think we're relating to other people. Actua...  Psychology\n",
            "49937  Award-winning Canadian author Kathleen Winter’...     Fiction\n"
          ]
        }
      ],
      "source": [
        "print(df[['book_desc', 'genre']].sample(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOEQ1SKtpoGc",
        "outputId": "6ed92ddb-fafa-4695-b655-aa2742e7b6e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                               book_desc              genre\n",
            "28767  Sixteen-year-old Tess's life has been shaped b...        Young Adult\n",
            "9010   Trollope's 1875 tale of a great financier's fr...           Classics\n",
            "40994  The magnificent Pulitzer Prize-winning novel o...            Fiction\n",
            "51942  If someone gave you a chair and said it was ma...  Christian Fiction\n",
            "41652                                                NaN          Sociology\n",
            "52897                       Book 32 of the Old Testament         Nonfiction\n",
            "7037   For almost two centuries, the stories of magic...           Classics\n",
            "8240   The execution-style murder of a Swedish housew...            Mystery\n",
            "24263  The Imperial Survey Service has four levels of...    Science Fiction\n",
            "52906  Deborah and Simon St. James have taken a holid...            Mystery\n"
          ]
        }
      ],
      "source": [
        "# Drop any rows with missing genre\n",
        "df = df.dropna(subset=['genre'])\n",
        "\n",
        "\n",
        "print(df[['book_desc', 'genre']].sample(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSilKUnQpvT7",
        "outputId": "04a99bd7-6d7c-4807-ff00-0f7e300b3506"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "genre\n",
            "Fantasy           7549\n",
            "Fiction           6828\n",
            "Romance           4412\n",
            "Young Adult       3711\n",
            "Nonfiction        2593\n",
            "                  ... \n",
            "Folk Tales           1\n",
            "Church               1\n",
            "Social Justice       1\n",
            "How To               1\n",
            "Pulp                 1\n",
            "Name: count, Length: 203, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(df['genre'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rc6dFDPqmL8"
      },
      "outputs": [],
      "source": [
        "df = df.dropna(subset=['book_desc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwF41gowqGhh"
      },
      "outputs": [],
      "source": [
        "top_genres = df['genre'].value_counts().index[:15]\n",
        "df = df[df['genre'].isin(top_genres)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0VPpS6CqamR",
        "outputId": "fda1bbda-363e-4014-d18a-415b3dbd1989"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(39211, 5000)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "# Fit and transform text\n",
        "X = vectorizer.fit_transform(df['book_desc']).toarray()\n",
        "\n",
        "print(X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duO0fiKvq9Ib",
        "outputId": "20755193-f8a9-4374-a37c-f0bcaacbe91f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[14  2  1  1 14  4  2  1  1  2]\n",
            "['Childrens' 'Classics' 'Fantasy' 'Fiction' 'Historical' 'History'\n",
            " 'Horror' 'Mystery' 'Nonfiction' 'Paranormal' 'Poetry' 'Romance'\n",
            " 'Science Fiction' 'Sequential Art' 'Young Adult']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(df['genre'])\n",
        "\n",
        "print(y[:10])\n",
        "print(encoder.classes_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n225QFuOqwhA",
        "outputId": "efe9e91f-b801-47a5-b60d-02faa23ce98f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(39211, 5000)\n",
            "[14  2  1  1 14  4  2  1  1  2]\n",
            "['Childrens' 'Classics' 'Fantasy' 'Fiction' 'Historical' 'History'\n",
            " 'Horror' 'Mystery' 'Nonfiction' 'Paranormal' 'Poetry' 'Romance'\n",
            " 'Science Fiction' 'Sequential Art' 'Young Adult']\n"
          ]
        }
      ],
      "source": [
        "print(X.shape)\n",
        "print(y[:10])\n",
        "print(encoder.classes_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfQlFOMS3Gdq"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgSZNGearOez",
        "outputId": "d838d36f-cb1a-4b3e-c0a0-bd0c5f9dafc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(31368, 5000) (7843, 5000)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(X_train.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "el6jgRhSBnTq",
        "outputId": "aabc7192-adff-413e-c92a-f6ff54bf40ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting PCA...\n",
            "Reduced shapes: (31368, 3853) (7843, 3853)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# PCA Dim Reduction\n",
        "\n",
        "print(\"Fitting PCA...\")\n",
        "n_components = 2500\n",
        "pca = PCA(n_components=0.95, random_state=42)\n",
        "X_train_reduced = pca.fit_transform(X_train)\n",
        "X_test_reduced = pca.transform(X_test)\n",
        "print(\"Reduced shapes:\", X_train_reduced.shape, X_test_reduced.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyVWOFRRcrj1",
        "outputId": "1408ba58-27ef-4d75-dfdc-474146f9e55c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total variance captured by 2500 components: 0.9501\n",
            "First 10 explained variance ratios: [0.0300226  0.01494424 0.01108106 0.00789455 0.00596185 0.00558847\n",
            " 0.00522296 0.0045341  0.00420509 0.00392758]\n"
          ]
        }
      ],
      "source": [
        "# Total variance captured (as a fraction of 1)\n",
        "total_variance_captured = np.sum(pca.explained_variance_ratio_)\n",
        "print(f\"Total variance captured by {n_components} components: {total_variance_captured:.4f}\")\n",
        "\n",
        "# Or view individual variance ratios\n",
        "print(\"First 10 explained variance ratios:\", pca.explained_variance_ratio_[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NW5KAMHwsTUw",
        "outputId": "a5989684-21e9-4b5b-eea5-667ab72547c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input dim: 3853, Num classes: 15\n"
          ]
        }
      ],
      "source": [
        "# Prepare NumPy Arrays\n",
        "\n",
        "X_train_np = X_train_reduced\n",
        "X_test_np = X_test_reduced\n",
        "y_train_np = np.array(y_train)\n",
        "y_test_np = np.array(y_test)\n",
        "\n",
        "num_samples, input_dim = X_train_np.shape\n",
        "num_classes = len(np.unique(y_train_np))\n",
        "print(f\"Input dim: {input_dim}, Num classes: {num_classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifOaUDt0sZk2"
      },
      "outputs": [],
      "source": [
        "\n",
        "hidden_dim1 = 512\n",
        "hidden_dim2 = 256\n",
        "learning_rate = 0.0001\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "l2_lambda = 0.001\n",
        "\n",
        "# Xavier Initialization\n",
        "\n",
        "def xavier_init(in_dim, out_dim):\n",
        "    return np.random.randn(in_dim, out_dim) * np.sqrt(1. / in_dim)\n",
        "\n",
        "W1 = xavier_init(input_dim, hidden_dim1)\n",
        "b1 = np.zeros((1, hidden_dim1))\n",
        "\n",
        "W2 = xavier_init(hidden_dim1, hidden_dim2)\n",
        "b2 = np.zeros((1, hidden_dim2))\n",
        "\n",
        "W3 = xavier_init(hidden_dim2, num_classes)\n",
        "b3 = np.zeros((1, num_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATI5Fiibsec9"
      },
      "outputs": [],
      "source": [
        "# Activation Functions\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy(preds, labels):\n",
        "    n_samples = preds.shape[0]\n",
        "    clipped_preds = np.clip(preds, 1e-12, 1. - 1e-12)\n",
        "    log_likelihood = -np.log(clipped_preds[range(n_samples), labels])\n",
        "    return np.mean(log_likelihood)\n",
        "\n",
        "# Adam Optimizer Variables\n",
        "\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "epsilon = 1e-8\n",
        "\n",
        "mW1, vW1 = np.zeros_like(W1), np.zeros_like(W1)\n",
        "mb1, vb1 = np.zeros_like(b1), np.zeros_like(b1)\n",
        "\n",
        "mW2, vW2 = np.zeros_like(W2), np.zeros_like(W2)\n",
        "mb2, vb2 = np.zeros_like(b2), np.zeros_like(b2)\n",
        "\n",
        "mW3, vW3 = np.zeros_like(W3), np.zeros_like(W3)\n",
        "mb3, vb3 = np.zeros_like(b3), np.zeros_like(b3)\n",
        "\n",
        "t = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pus91nbsnp9",
        "outputId": "07be65ab-906b-4214-c5b4-22a53a600539"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 1/100, Avg Loss: 2.6386\n",
            "Epoch 2/100, Avg Loss: 2.0838\n",
            "Epoch 3/100, Avg Loss: 1.8139\n",
            "Epoch 4/100, Avg Loss: 1.6554\n",
            "Epoch 5/100, Avg Loss: 1.5447\n",
            "Epoch 6/100, Avg Loss: 1.4707\n",
            "Epoch 7/100, Avg Loss: 1.4166\n",
            "Epoch 8/100, Avg Loss: 1.3759\n",
            "Epoch 9/100, Avg Loss: 1.3441\n",
            "Epoch 10/100, Avg Loss: 1.3186\n",
            "Epoch 11/100, Avg Loss: 1.2968\n",
            "Epoch 12/100, Avg Loss: 1.2788\n",
            "Epoch 13/100, Avg Loss: 1.2635\n",
            "Epoch 14/100, Avg Loss: 1.2503\n",
            "Epoch 15/100, Avg Loss: 1.2382\n",
            "Epoch 16/100, Avg Loss: 1.2273\n",
            "Epoch 17/100, Avg Loss: 1.2175\n",
            "Epoch 18/100, Avg Loss: 1.2088\n",
            "Epoch 19/100, Avg Loss: 1.2006\n",
            "Epoch 20/100, Avg Loss: 1.1929\n",
            "Epoch 21/100, Avg Loss: 1.1862\n",
            "Epoch 22/100, Avg Loss: 1.1794\n",
            "Epoch 23/100, Avg Loss: 1.1739\n",
            "Epoch 24/100, Avg Loss: 1.1681\n",
            "Epoch 25/100, Avg Loss: 1.1632\n",
            "Epoch 26/100, Avg Loss: 1.1585\n",
            "Epoch 27/100, Avg Loss: 1.1535\n",
            "Epoch 28/100, Avg Loss: 1.1490\n",
            "Epoch 29/100, Avg Loss: 1.1443\n",
            "Epoch 30/100, Avg Loss: 1.1409\n",
            "Epoch 31/100, Avg Loss: 1.1372\n",
            "Epoch 32/100, Avg Loss: 1.1330\n",
            "Epoch 33/100, Avg Loss: 1.1297\n",
            "Epoch 34/100, Avg Loss: 1.1258\n",
            "Epoch 35/100, Avg Loss: 1.1227\n",
            "Epoch 36/100, Avg Loss: 1.1194\n",
            "Epoch 37/100, Avg Loss: 1.1160\n",
            "Epoch 38/100, Avg Loss: 1.1128\n",
            "Epoch 39/100, Avg Loss: 1.1094\n",
            "Epoch 40/100, Avg Loss: 1.1066\n",
            "Epoch 41/100, Avg Loss: 1.1043\n",
            "Epoch 42/100, Avg Loss: 1.1007\n",
            "Epoch 43/100, Avg Loss: 1.0977\n",
            "Epoch 44/100, Avg Loss: 1.0954\n",
            "Epoch 45/100, Avg Loss: 1.0920\n",
            "Epoch 46/100, Avg Loss: 1.0892\n",
            "Epoch 47/100, Avg Loss: 1.0868\n",
            "Epoch 48/100, Avg Loss: 1.0839\n",
            "Epoch 49/100, Avg Loss: 1.0814\n",
            "Epoch 50/100, Avg Loss: 1.0794\n",
            "Epoch 51/100, Avg Loss: 1.0759\n",
            "Epoch 52/100, Avg Loss: 1.0745\n",
            "Epoch 53/100, Avg Loss: 1.0720\n",
            "Epoch 54/100, Avg Loss: 1.0697\n",
            "Epoch 55/100, Avg Loss: 1.0677\n",
            "Epoch 56/100, Avg Loss: 1.0654\n",
            "Epoch 57/100, Avg Loss: 1.0627\n",
            "Epoch 58/100, Avg Loss: 1.0601\n",
            "Epoch 59/100, Avg Loss: 1.0584\n",
            "Epoch 60/100, Avg Loss: 1.0562\n",
            "Epoch 61/100, Avg Loss: 1.0541\n",
            "Epoch 62/100, Avg Loss: 1.0514\n",
            "Epoch 63/100, Avg Loss: 1.0495\n",
            "Epoch 64/100, Avg Loss: 1.0478\n",
            "Epoch 65/100, Avg Loss: 1.0459\n",
            "Epoch 66/100, Avg Loss: 1.0436\n",
            "Epoch 67/100, Avg Loss: 1.0412\n",
            "Epoch 68/100, Avg Loss: 1.0389\n",
            "Epoch 69/100, Avg Loss: 1.0368\n",
            "Epoch 70/100, Avg Loss: 1.0352\n",
            "Epoch 71/100, Avg Loss: 1.0325\n",
            "Epoch 72/100, Avg Loss: 1.0312\n",
            "Epoch 73/100, Avg Loss: 1.0290\n",
            "Epoch 74/100, Avg Loss: 1.0263\n",
            "Epoch 75/100, Avg Loss: 1.0251\n",
            "Epoch 76/100, Avg Loss: 1.0233\n",
            "Epoch 77/100, Avg Loss: 1.0212\n",
            "Epoch 78/100, Avg Loss: 1.0185\n",
            "Epoch 79/100, Avg Loss: 1.0171\n",
            "Epoch 80/100, Avg Loss: 1.0157\n",
            "Epoch 81/100, Avg Loss: 1.0135\n",
            "Epoch 82/100, Avg Loss: 1.0118\n",
            "Epoch 83/100, Avg Loss: 1.0098\n",
            "Epoch 84/100, Avg Loss: 1.0075\n",
            "Epoch 85/100, Avg Loss: 1.0057\n",
            "Epoch 86/100, Avg Loss: 1.0042\n",
            "Epoch 87/100, Avg Loss: 1.0026\n",
            "Epoch 88/100, Avg Loss: 1.0004\n",
            "Epoch 89/100, Avg Loss: 0.9986\n",
            "Epoch 90/100, Avg Loss: 0.9969\n",
            "Epoch 91/100, Avg Loss: 0.9947\n",
            "Epoch 92/100, Avg Loss: 0.9937\n",
            "Epoch 93/100, Avg Loss: 0.9912\n",
            "Epoch 94/100, Avg Loss: 0.9898\n",
            "Epoch 95/100, Avg Loss: 0.9874\n",
            "Epoch 96/100, Avg Loss: 0.9863\n",
            "Epoch 97/100, Avg Loss: 0.9844\n",
            "Epoch 98/100, Avg Loss: 0.9827\n",
            "Epoch 99/100, Avg Loss: 0.9809\n",
            "Epoch 100/100, Avg Loss: 0.9792\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Training Loop\n",
        "loss_history = []\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    perm = np.random.permutation(num_samples)\n",
        "    X_shuffled = X_train_np[perm]\n",
        "    y_shuffled = y_train_np[perm]\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in range(0, num_samples, batch_size):\n",
        "        X_batch = X_shuffled[i:i+batch_size]\n",
        "        y_batch = y_shuffled[i:i+batch_size]\n",
        "        bs = X_batch.shape[0]\n",
        "\n",
        "        # Forward pass\n",
        "        z1 = np.dot(X_batch, W1) + b1\n",
        "        a1 = relu(z1)\n",
        "\n",
        "        z2 = np.dot(a1, W2) + b2\n",
        "        a2 = relu(z2)\n",
        "\n",
        "        z3 = np.dot(a2, W3) + b3\n",
        "        preds = softmax(z3)\n",
        "\n",
        "        # Loss with L2\n",
        "        loss = cross_entropy(preds, y_batch)\n",
        "        loss += (l2_lambda/2) * (np.sum(W1**2) + np.sum(W2**2) + np.sum(W3**2))\n",
        "        total_loss += loss * bs\n",
        "\n",
        "        # Backward pass\n",
        "        dZ3 = preds\n",
        "        dZ3[range(bs), y_batch] -= 1\n",
        "        dZ3 /= bs\n",
        "\n",
        "        dW3 = np.dot(a2.T, dZ3) + l2_lambda * W3\n",
        "        db3 = np.sum(dZ3, axis=0, keepdims=True)\n",
        "\n",
        "        dA2 = np.dot(dZ3, W3.T)\n",
        "        dZ2 = dA2 * relu_derivative(z2)\n",
        "\n",
        "        dW2 = np.dot(a1.T, dZ2) + l2_lambda * W2\n",
        "        db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "        dA1 = np.dot(dZ2, W2.T)\n",
        "        dZ1 = dA1 * relu_derivative(z1)\n",
        "\n",
        "        dW1 = np.dot(X_batch.T, dZ1) + l2_lambda * W1\n",
        "        db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "        # Adam updates\n",
        "        t += 1\n",
        "        for param, grad, m, v in [\n",
        "            (W1, dW1, mW1, vW1),\n",
        "            (b1, db1, mb1, vb1),\n",
        "            (W2, dW2, mW2, vW2),\n",
        "            (b2, db2, mb2, vb2),\n",
        "            (W3, dW3, mW3, vW3),\n",
        "            (b3, db3, mb3, vb3),\n",
        "        ]:\n",
        "            m[:] = beta1 * m + (1 - beta1) * grad\n",
        "            v[:] = beta2 * v + (1 - beta2) * (grad ** 2)\n",
        "            m_hat = m / (1 - beta1 ** t)\n",
        "            v_hat = v / (1 - beta2 ** t)\n",
        "            param -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
        "\n",
        "    avg_loss = total_loss / num_samples\n",
        "    loss_history.append(avg_loss)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Avg Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "QLyIuovPtQg_",
        "outputId": "200598ee-1af5-4aa0-8e32-c38458a13162"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'epochs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-3671010297.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Average Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'epochs' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, epochs + 1), loss_history, marker='o')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Average Loss')\n",
        "plt.title('Training Loss Curve')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3I6a0cGBsxLc",
        "outputId": "0a3a8f76-d228-4746-c0c1-fc0a301a9bed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating on test set...\n",
            "\n",
            "Test Accuracy: 60.47%\n",
            "\n",
            "Classification Report:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "      Childrens       0.55      0.45      0.49       217\n",
            "       Classics       0.51      0.46      0.49       403\n",
            "        Fantasy       0.67      0.72      0.70      1498\n",
            "        Fiction       0.55      0.62      0.58      1348\n",
            "     Historical       0.62      0.50      0.55       477\n",
            "        History       0.67      0.59      0.62       186\n",
            "         Horror       0.59      0.41      0.48       210\n",
            "        Mystery       0.61      0.57      0.59       416\n",
            "     Nonfiction       0.64      0.68      0.66       511\n",
            "     Paranormal       0.57      0.38      0.46       120\n",
            "         Poetry       0.80      0.55      0.65       166\n",
            "        Romance       0.66      0.69      0.68       875\n",
            "Science Fiction       0.59      0.51      0.55       379\n",
            " Sequential Art       0.56      0.50      0.53       298\n",
            "    Young Adult       0.53      0.60      0.56       739\n",
            "\n",
            "       accuracy                           0.60      7843\n",
            "      macro avg       0.61      0.55      0.57      7843\n",
            "   weighted avg       0.61      0.60      0.60      7843\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[  97    7   23   23    2    1    1    6   10    0    0    5    2   16\n",
            "    24]\n",
            " [   6  186   21  122   10    7    2    4   19    0    6    6    4    4\n",
            "     6]\n",
            " [  18   22 1084   57   24    1   13   19   16   16    1   72   33   19\n",
            "   103]\n",
            " [  13   73   95  834   51    6    8   45   48    0    5   66   26   29\n",
            "    49]\n",
            " [   9   15   29   94  237    7    1   12    7    0    2   27    8    7\n",
            "    22]\n",
            " [   0    7    5    9    8  109    0    0   43    0    3    0    2    0\n",
            "     0]\n",
            " [   2    5   30   30    2    0   86    6    7    1    2    6   13    3\n",
            "    17]\n",
            " [   2    7   14   68    8    0    8  239    5    2    1   25    1    9\n",
            "    27]\n",
            " [   8   11   15   69    7   27    1    4  346    0    0    5    5    4\n",
            "     9]\n",
            " [   1    1   31    0    0    0    3    1    1   46    0   13    2    1\n",
            "    20]\n",
            " [   2   17    6   24    0    1    1    0   16    0   91    0    1    3\n",
            "     4]\n",
            " [   1    5   79   58   19    0    1   25    6    6    0  601    8    5\n",
            "    61]\n",
            " [   3    3   54   45    3    3    5    8    4    2    0   12  192   10\n",
            "    35]\n",
            " [   5    1   53   21    5    0    7    5    6    1    2   13   11  149\n",
            "    19]\n",
            " [  10    2   81   69    8    1    9   19   10    7    1   53   15    8\n",
            "   446]]\n"
          ]
        }
      ],
      "source": [
        "# Evaluate\n",
        "\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "z1_test = np.dot(X_test_np, W1) + b1\n",
        "a1_test = relu(z1_test)\n",
        "\n",
        "z2_test = np.dot(a1_test, W2) + b2\n",
        "a2_test = relu(z2_test)\n",
        "\n",
        "z3_test = np.dot(a2_test, W3) + b3\n",
        "preds_test = softmax(z3_test)\n",
        "y_pred = np.argmax(preds_test, axis=1)\n",
        "\n",
        "accuracy = np.mean(y_pred == y_test_np)\n",
        "print(f\"\\nTest Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_np, y_pred, target_names=encoder.classes_))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test_np, y_pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMxyiKcyerT3yHKOye7e4Y"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}